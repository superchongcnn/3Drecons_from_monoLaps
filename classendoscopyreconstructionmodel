import torch
import torch.nn as nn

class EndoscopyReconstructionModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads, num_layers):
        super().__init__()
        
        # 1. Feature Extraction
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        
        # 2. Time
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )
        
        # 3. Self_attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim * 2,  # 双向LSTM输出
            num_heads=num_heads
        )
        
        # 4. Depth_Estimation
        self.depth_estimator = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # 5. 3D_pointcloud
        self.point_cloud_generator = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 3)  # x, y, z coordinates
        )
        
    def forward(self, image_sequence):
        batch_size, seq_len, c, h, w = image_sequence.shape
        
        # 1. extract features from key frame
        features = []
        for t in range(seq_len):
            frame_features = self.feature_extractor(image_sequence[:, t])
            features.append(frame_features.flatten(start_dim=1))
        features = torch.stack(features, dim=1)  # [batch_size, seq_len, feature_dim]
        
        # 2. time module
        lstm_out, _ = self.lstm(features)
        
        # 3. self_attention
        attention_out, _ = self.self_attention(
            lstm_out.transpose(0, 1),
            lstm_out.transpose(0, 1),
            lstm_out.transpose(0, 1)
        )
        attention_out = attention_out.transpose(0, 1)
        
        # 4. depthestimation
        depth_maps = self.depth_estimator(attention_out)
        
        # 5. 3d_pointcloud
        point_cloud = self.point_cloud_generator(attention_out)
        
        return {
            'depth_maps': depth_maps,
            'point_cloud': point_cloud,
            'attention_weights': _
        }

def train_reconstruction_model(model, train_loader, num_epochs, device):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters())
    
    for epoch in range(num_epochs):
        for batch_idx, (images, targets) in enumerate(train_loader):
            images = images.to(device)
            targets = targets.to(device)
            
            # forward
            outputs = model(images)
            loss = criterion(outputs['depth_maps'], targets['depth'])
            loss += criterion(outputs['point_cloud'], targets['points'])
            
            # backward_and_optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
